---
title: "Tidy Data"
author: "Paul Oldham"
date: "19 August 2015"
output: html_document
---
```{r global_options, include=FALSE}
knitr::opts_chunk$set(cache=TRUE, tidy=TRUE, fig.width=12, fig.height=8, fig.align = "center", echo=TRUE, warning=FALSE, message=FALSE)
```
Infographics are an increasingly popular way of presenting data in a way that is easy for the reader to understand without reading a long report. Infographics are particularly well suited to presenting data with simple messages about the key findings. A good infographic can encourage the audience to read the full report or article and should be seen as a complement to a substantive piece of analysis rather than a replacement. 

Some patent offices have already been creating infographics as part of their reports to policy makers or to clients. The Instituto Nacional de Propiedade Industrial (INPI) in Brazil produces regular two page [Technology Radar](http://www.inpi.gov.br/menu-servicos/informacao/radares-tecnologicos) (Radar Tecnologico) consisting of charts and maps that briefly summarise more detailed research on subjects such as [Nanotechnology in Waste Management](http://www.inpi.gov.br/menu-servicos/arquivos-cedin/n08_radar_tecnologico_nano_residuos_versao_resumida_ingles_atualizada_20160122.pdf). [WIPO Patent Landscape Reports](http://www.wipo.int/patentscope/en/programs/patent_landscapes/), which go into depth on patent activity for a particular area, are accompanied by one page infographics that have proved very popular such as the infographic accompanying a recent report on [assistive devices](http://www.wipo.int/export/sites/www/patentscope/en/programs/patent_landscapes/reports/documents/assistivedevices_infographic.pdf). 

A growing number of companies are offering online infographic software services such as [infogr.am](https://infogr.am/app/#/library),[easel.ly](http://www.easel.ly) [piktochart.com](https://magic.piktochart.com/templates), [canva.com](https://www.canva.com/create/infographics/) or [venngage.com](https://venngage.com) to mention only a selection of the offerings out there. The [Cool Infographics website](http://www.coolinfographics.com/tools/) provides a useful overview of available tools. 

One feature of many of these services is that they are based on a freemium model. Creating graphics is free but the ability to export files and the available formats for export of your masterpiece (e.g. high resolution or .pdf) often depend on upgrading to a monthly account at varying prices. In this chapter we test drive [infogr.am](https://infogr.am/app/#/library) as a chart friendly service, albeit with export options that depend on a paid account. 

In the first part of the chapter we will build on work in other chapter using R to prepare a patent dataset for visualisation with [infogr.am](https://infogr.am/app/#/library). In particular we focus on ways of breaking up concatenated fields in patent data (e.g. applicants, IPCs) to allow to tidy patent data for visualisation in an infographic. Our focus is on organising data for visualisation using R rather than cleaning data with tools such as Google Refine covered elsewhere in the Manual. As with other chapters in the Manual we do not assume any prior knowledge of R and everything you need to know is in this chapter. However, earlier chapters cover some topics of working with data in R in greater detail. 

In the second part of the chapter we will focus on visualing the data we have exported to develop a storyline using patent data that can then be shared with others.

##Installing packages and Loading Libraries

To start with we need to ensure that RStudio and R for your operating system are installed by following the instructions on the RStudio website [here](https://www.rstudio.com/products/rstudio/download/). Do not forget to follow the link to also [install R for your operating system](https://cran.rstudio.com).

R works using packages (libraries) and there are around 7,490 of them for a whole range of purposes. We will use just a few of them. To install a package we use the following. Copy and paste the code into the Console and press enter.  

```{r eval=FALSE}
install.packages("readr") # read in .csv files `readxl` for excel files
install.packages("dplyr") # wrangle data
install.packages("tidyr") # tidy data
install.packages("stringr") # work with text strings
```

Packages can also be installed by selecting the Packages tab and typing the name of the package. 

To load the package (library) use the following or check the tick box in the Packages pane. 
 
```{r libraries}
library(readr) 
library(dplyr) 
library(tidyr) 
library(stringr)
```

We are now ready to go. 

##Load a .csv file using `readr`

We will work with the `pizza_medium_clean` dataset in the online [Github Manual repository](https://github.com/poldham/opensource-patent-analytics/tree/master/2_datasets). If manually downloading a file remember to click on the file name and select `Raw` to download the actual file. 

We can use the easy to use `read_csv()` function from the `readr` package to quickly read in our pizza data directly from the Github repository.  

```{r}
pizza <- read_csv("/Users/pauloldham17inch/opensource-patent-analytics/2_datasets/pizza_medium_clean/pizza.csv")
```

`readr` will display a warning for the file arising from its efforts to parse publication dates on import. We will ignore this as we will not be using this field.

As an alternative to importing directly from Github download the file and enter the path in quotes (you must use the full path, e.g. C: etc).

```{r}
pizza_read <- read_csv("yourfilepath")
```

`readr` and `readxl` (for Excel files) are quite new. For more complex data see the Manual articles on the [`readr`](http://poldham.github.io/reading-csv-files-in-R/) and [`readxl`](http://poldham.github.io/reading-writing-excel-files-R/) packages for importing Excel.

##Viewing Data

We can view data in a variety of ways. 

1. In the console

```{r}
pizza
```

2. In environment click on the blue arrow to see in the environment. Keep clicking to open a new window with the data. 

3. Use the `View()` command (for data.frames and tables)

```{r}
View(pizza)
```

If possible use the View() command or environment. The difficulty with the console is that large amounts of data will simply stream past. 

##Identifying Types of Object

We often want to know what type of object we are working with and more details about the object so we know what to do later. Here are some of the most common commands for obtaining information about objects.

```{r eval=FALSE}
class(pizza) ## type of object
names(pizza) ## names of variables
str(pizza) ## structure of object
dim(pizza) ## dimensions of the object
```

The most useful command in this list is `str()` because this allows us to access the structure of the object and see its type

```{r}
str(pizza)
```

This will tell us that the pizza object has three classes (most have one) but this is not a bad thing. We will also see the names of the fields (vectors) and their type. Most patent data is a character vector with dates forming integers. 

##Working with Data

We will often want to select aspects of our data to focus on a specific set of columns or to create a graph. We might also want to add information, such as numeric count. 

The `dplyr` package provides a set of very handy functions for selecting, adding and counting data. 

###Select

In this case we will start by using the `select()` function to limit the data to specific columns. We can do this using their names or their numeric position (best for large number of columns e.g. 1:31). In `dplyr` existing character columns do not require `""`.

```{r}
total <- select(pizza, publication_number, publication_year)
```

We now have a new data.frame that contains two columns. One with the year and one with the publication number. Note that we have created a new object called total using `<-` and that after select we have named our original data and the columns we want. A fundamental feature of select is that it will drop columns that you do not name. So it is best to create a new object if you want to keep your original data for later work. 

###Adding data with `mutate()`

`mutate()` is a `dplyr` function that allows us to add data based on existing data in our data frame, for example to perform a calculation. In the case of patent data we normally lack a numeric field to use for counts. We can however assign a value to our publication field by using sum() and the number 1 as follows. 

```{r}
total <- mutate(total, n = sum(publication_number = 1))
```

When we view total we now have a value of 1 in the column `n` for each publication number. Note that in some cases a publication or family number may occur multiple times and we would want to reduce the dataset to distinct records. For that we would use `n_distinct()` from `dplyr` or `unique()` from base R but we will continue as is for the moment.

###Counting data using `count()`

At the moment, we have multiple instances of the same year (where a patent publication occurs in that year). We now want to calculate how many of our documents were published in each year. To do that we will use the `dplyr` function `count()`. We will use the publication_year and add `wt = ` (for weight) with n as the value to count.   

```{r}
total <- count(total, publication_year, wt = n)
```

When we now examine total, we will see the publication year and a summed value for the records in that year. 

###Renaming a field using `rename()`

Next we will use rename from `dplyr` to rename the fields. Note that understanding which field require quote marks can take some effort. In this case renaming the character vector publication_year as "pubyear" requires quotes while renaming the numeric vector does not. 

```{r}
total <- rename(total, "pubyear" = publication_year, publications = n)
```

###Make a quickplot with `qplot()`

Using the `qplot()` function in ggplot2 we can now draw a quick line graph. Note that qplot() is unusual in R because the data (total) appears after the coordinates. We will specify that we want a line using `geom =` (if geom is left out it will be a scatter plot) and limit the x axis using xlim to take out the data cliff that is normal with patent data due to a lack of complete records for recent years. 

```{r}
qplot(x = pubyear, y = publications, data = total, geom = "line", xlim = c(1940,2012))
```

For more details on graphing in R see the [qplot](http://poldham.github.io/ggplot_pizza_patents_part1/) and [gglot2](http://poldham.github.io/ggplot_pizza_patents_part2j/) articles. 

###Simplify code using pipes `%>%`

So far we have handled the code one line at a time. But, one of the great strengths of using a programming language is that we can run multiple lines of code together. There are two basic ways that we can do this

1. The standard or old fashioned way

```{r}
total <- select(pizza, publication_number, publication_year)
total <- mutate(total, n = sum(publication_number = 1))
total <- count(total, publication_year, wt = n)
total <- rename(total, "pubyear" = publication_year, publications = n)
qplot(x = pubyear, y = publications, data = total, geom = "line", xlim = c(1940,2012))
```

The code we have just created is four lines long. We could select all of this code and run it in one go in the console. Try it (you must have imported pizza first).

2. Using pipes `%>%`

An alternative way to do this is to use pipes that were first introduced in the `magrittr` package and then picked up in dplyr and tidyr. Pipes are now very popular because they simplify writing R code and speed it up. The most popular pipe is %>% which means "this then that". 

```{r}
df <- 
  select(pizza, publication_number, publication_year) %>%
  mutate(n = sum(publication_number = 1)) %>%
  count(publication_year, wt = n) %>%
  rename("pubyear" = publication_year, publications = n)
print(df)
```

In this case we have created a new object (df for data.frame) and then the code. Note that we have not created and then over-written the total object at each point. Note also that with the exception of the first line we have not had to name the source data as the first expression in the functions. As a result it is easier to read and to understand.

In this case, we have not added the call to qplot but we will do so now.  

```{r}
dfplot <- 
  select(pizza, publication_number, publication_year) %>%
  mutate(n = sum(publication_number = 1)) %>%
  count(publication_year, wt = n) %>%
  rename("pubyear" = publication_year, publications = n)
qplot(x = pubyear, y = publications, dfplot, geom = "line", xlim = c(1940,2012))
```

##Tidying data - Separating and Gathering

In patent data we often see concatenated fields with a separator (normally a ;). These are typically applicant names, inventor names, IPC codes, or document numbers (priority numbers, family member numbers). To work with this data we will typically need to do two things. 

1. Separate the data so that each name is distinct. This normally involves separating into columns
2. Gathering the data back in. This involves transforming the data in columns into rows. 

The tidyr package contains two functions that are very useful when working with patent data. The first of these is separate. 

Here we will work with the applicants_cleaned field in the pizza dataset. This field contains concatenated names with a `;` as the separator. The first issue we will encounter is that we do not know how many names might be in the data. One option is to use an arbitrarily high number.

###Separate

In the first step we use the tidyr `separate()` function. Ideally the field we want to separate is the first column because we will be using the numeric positions of the columns. In the case of the pizza data the applicants_cleaned field is already the first column. If we wanted to move a column to the first position we could use select() as follows. To illustrate this we will create an object called pizza1.

```{r}
pizza1 <- select(pizza, 2:31, 1) #moves column 1 to the end (column 31)
pizza2 <- select(pizza1, 31, 1:30) #moves column 31 to the first column
```

Next we use select(), we begin by creating df2, with pizza as the data that we want to use. This is followed by the unquoted name of the column and the number of columns we want to separate the applicants into (1:30). We then specify the separator with the `;`. The next two arguments are for what to do with any extra data and the direction to fill cells. We use `fill = "right"` because separate will throw an error if the pieces are not all of the same size. 

```{r}
df2 <- separate(pizza, applicants_cleaned, 1:30, sep = ";", extra = "merge", fill = "right") 
```

Note that while this works there is some inconsistency where the underlying data has a semicolon as the separator where it should be a `,`. As a result some of the names will be incorrectly split. We will simply live with this for the time being. 

The second step is to use the `tidyr()` function `gather()`. This will gather the columns we specify into rows. `gather()` involves specifying a key value pair. We can introduce the key (a numeric value) if we don't have one by specifying a column name and gather will create it for us. In this case we use `n`. Then we specify the value - the column that we want to gather the names into - that we will call applicants. Then we specify the columns to gather by their numeric position. Finally, where there are NA (Not Available) values we specify `na.rm = TRUE` to remove them. 

```{r}
df2 <- gather(df2, n, applicants, 1:30, na.rm = TRUE) %>%
 print() #check this
```

We now have a data.frame with 32 columns and 14,461 rows. If we use View(df2) we will see that tidyr has created our applicants column at the end of the data.frame (column 32).

However, if we now inspect the column by subsetting into it using $ we will see that a lot of the names have a leading whitespace space. This results from the separate exercise where the ; is actually `;space`. Take a look at the data. 

```{r}
df2$applicants
```

We can address this using a function from the `stringr` package `str_trim()`. We have a choice with `str_trim()` on whether to trim the whitespace on the right, left or both. Here we have chosen both. Trimming whitespace is important because it affects how names will rank at a later stage. For example " Dibble, James W" will be treated as a separate name from "Dibble, James W". 

Because we are seeking to modify an existing column (not to create a new vector or data.frame) we will use `$` in the object and as the data for the str_trim function. 

```{r}
df2$applicants <- str_trim(df2$applicants, side = "both")
```

We can tie these steps together using pipes into the following simpler code. 

```{r}
df2 <- separate(pizza, applicants_cleaned, 1:30, sep = ";", extra = "merge", fill = "right") %>% gather(n, applicants, 1:30, na.rm = TRUE) 
df2$applicants <- str_trim(df2$applicants, side = "both")
#could be
df2 %>% str_trim(applicants, side = "both")
```

Note that when using `str_trim()` we use subsetting to modify the applicants column in place. There is possibly a more efficient way of doing this with pipes but this appears difficult because the data.frame needs to exist for the `str_trim()` to act on in place or we end up with a vector of applicant names rather than a data.frame.   

We now have some working code that will separate out our names, gather it back in and then trim it. However, it would be very helpful if we knew the maximum number of names that the applicants, inventors or IPC code field breaks into in a given dataset. The code below is a small function that starts by counting the number of separators (sep) in a column (col) using the `str_count()` function from `stringr`. In this case some of the fields are NA. In R, where a vector contains NA values R will `always` return NA as the answer. So, we use na.omit() to remove NA from the calculation (note that we are using pipes so we name data only once). Then we create a separate object that calculates the maximum value (max ()). We need to oblige R to do this as an integer by placing the max() function inside as.integer. 

The final name in the concatenated applicants name will not possess a separator at the end. If we don't address this then our function will undercount the names. A simple way to accomodate this is to add +1 at the end of the calculation.  

```{r counting names}
library(stringr)
library(dplyr)
actor_count <- function(data, col = "", sep = "[^[:alnum:]]+") {
  actor_count <- str_count(data[[col]], pattern = sep) %>%
    na.omit()
  n <- as.integer(max(actor_count) + 1) %>%
  print()
}
```

Copy and paste the above code into the console to make the function available. Head to the Environment tab and you should see it in the fuctions.

```{r}
n <- actor_count(pizza, "applicants_cleaned", sep = ";")
```

We can now rerun our original code and instead of using an arbitrary number we can use the value of `n`. 

```{r}
df3 <- separate(pizza, applicants_cleaned, 1:n, sep = ";", extra = "merge", fill = "right") %>%
  gather(n, applicants, 1:n, na.rm = TRUE) 
df3$applicants <- str_trim(df3$applicants, side = "both")
```

##Selecting applicants using `filter()`

Wheras select functions with columns, `filter()` from dplyr works with rows. Here we will filter the data to select the rows in the applicants column that contain Google Inc. 

```{r}
google <- filter(df3, applicants == "Google Inc")
```

Note that the correct result (191 records) will only be achieved where you used the separated and trimmed data we created in df3. 

<!---new heading here--->

```{r}
phrase_google <- select(google, title_nlp_multiword_phrases, publication_number) %>%
  separate(title_nlp_multiword_phrases, 1:30, sep = ";", fill = "right") %>%
  mutate(n = sum(publication_number = 1)) %>%
  select(1:30, 32) %>%
  gather(x, phrases, 1:30, na.rm = TRUE) 
phrase_google$phrases <- str_trim(phrase_google$phrases, side = "both")

#now we sum up the trimmed data and arrange in descending order
phrase_google1 <- select(phrase_google, phrases, n) %>%
  count(phrases, wt = n) %>%
  arrange(desc(n)) %>%
  filter(n >= 3)
write_csv(phrase_google, "google_phrases.csv") #write to a .csv file
```

#Spreading data using spread()

There are two main data formats: Long and wide. Long data is typically the aim of tidying data where observations (values) of the same variables (publication numbers, years, country names etc.) are grouped into the same columns. However, quite a number of applications may expect the data to be in wide format. In this case each country or year would appear as a separate column. 

In this example we will take some long data and convert it into a wide format for use with [infogr.am](https://infogr.am/). 

In using spread note that it takes a data argument (pizza) and a key, and value column. In this case the data is named in the first line and does not need to be named again. For additional arguments see ?spread(). In this case we are specifying that we want to spread the country names (as the key) with `n` as the value. The other workings before spread will be familiar by now.  

```{r}
country_totals <- select(pizza, publication_country_name, publication_number, publication_year) %>%
  mutate(n = sum(publication_number = 1)) %>% 
  count(publication_country_name, publication_year, wt = n) %>%
  spread(publication_country_name, n)
write_csv(pcy_spread, "country_totals.csv")
```



##IPC data

```{r}
ipc_detail <- select(pizza, ipc_subclass_detail, publication_number) %>%
  mutate(n = sum(publication_number = 1)) %>% 
  count(ipc_subclass_detail, wt = n) %>%
  filter(n)

%>%
  spread(ipc_subclass_detail, n)
write_csv(pcy_spread, "country_totals.csv")
```









###Workings (ignore)

```{r}
app_plot <- 
  select(df3, publication_number, applicants) %>%
  mutate(n = sum(publication_number = 1)) %>%
  count(applicants, wt = n) %>%
  arrange(desc(n)) %>%
  print()
```
